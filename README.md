# ClipCrop: Conditioned Cropping Driven by Vision-Language Model

It was fortunate to find the missing annotated data for the GAIC dataset on my old laptop (see directory "GAIC-Text-Annotations").  
Hopefully these annotations will be useful to the field of smart cropping.  
I personally don't follow this field, so there will be no updates.  

Download the GAIC dataset (journal version) from [here](https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch), which includes 2636, 200, and 500 images for training, validation and testing.

If you find this repository useful, please consider citing:

```bibtex
@inproceedings{zhong2023clipcrop,
  title={ClipCrop: conditioned cropping driven by vision-language model},
  author={Zhong, Zhihang and Cheng, Mingxi and Wu, Zhirong and Yuan, Yuhui and Zheng, Yinqiang and Li, Ji and Hu, Han and Lin, Stephen and Sato, Yoichi and Sato, Imari},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={294--304},
  year={2023}
}
```